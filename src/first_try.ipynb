{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "unet/diffusion_pytorch_model.safetensors not found\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c04a862e62744ba682e90acf1c8bde62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tim.kuipers/mambaforge-pypy3/envs/diffusers_env/lib/python3.11/site-packages/transformers/models/clip/feature_extraction_clip.py:28: FutureWarning: The class CLIPFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use CLIPImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from diffusers import DiffusionPipeline\n",
    "import torch\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "repo_id = \"Lykon/DreamShaper\"\n",
    "pipe = DiffusionPipeline.from_pretrained(repo_id)\n",
    "pipe = pipe.to(device)\n",
    "\n",
    "\n",
    "pipe.safety_checker = None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size: (970, 546)\n",
      "Shape: torch.Size([3, 512, 1024])\n"
     ]
    }
   ],
   "source": [
    "img_URL = 'https://tonedeaf.thebrag.com/wp-content/uploads/2018/07/aphex-twin.jpg'\n",
    "# download image\n",
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "in_image = Image.open(BytesIO(requests.get(img_URL).content))\n",
    "print(f'Size: {in_image.size}')\n",
    "in_image = in_image.resize((1024, 512))\n",
    "# convert to torch tensor\n",
    "import torchvision.transforms as transforms\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "in_image = preprocess(in_image)\n",
    "in_image = in_image.to(device)\n",
    "print(f'Shape: {in_image.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a42eb42860e94892b0208d382d34e990",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tim.kuipers/mambaforge-pypy3/envs/diffusers_env/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from IPython.display import display, clear_output\n",
    "import time\n",
    "\n",
    "\n",
    "'''\n",
    "# Example: You can use torchvision transforms to convert the tensor to a format suitable for display\n",
    "transform = transforms.ToPILImage()\n",
    "image_pil = transform(torch.randn(1, 3, 512, 1024)[0])\n",
    "\n",
    "# Display the initial image\n",
    "plt.figure(figsize=(8, 8))\n",
    "img_plot = plt.imshow(image_pil)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "'''\n",
    "\n",
    "from torchvision.models.optical_flow import raft\n",
    "from torchvision.models.optical_flow import Raft_Large_Weights\n",
    "import torchvision.transforms.functional as vision_F\n",
    "\n",
    "weights = Raft_Large_Weights.DEFAULT\n",
    "flow_model = raft.raft_large(weights=weights, progress=False).to(device)\n",
    "flow_model = flow_model.eval()\n",
    "\n",
    "flow_stage_num = 1 # 0-11\n",
    "\n",
    "transforms = weights.transforms()\n",
    "def preprocess(img1_batch, img2_batch):\n",
    "    img1_batch = vision_F.resize(img1_batch, size=[520, 960], antialias=False)\n",
    "    img2_batch = vision_F.resize(img2_batch, size=[520, 960], antialias=False)\n",
    "    return transforms(img1_batch.unsqueeze(0), img2_batch.unsqueeze(0))\n",
    "\n",
    "\n",
    "\n",
    "def warp_image(image, flow):\n",
    "    h, w, c = image.shape\n",
    "    x = torch.arange(w, device=device).float().view(1, -1).expand(h, -1)\n",
    "    y = torch.arange(h, device=device).float().view(-1, 1).expand(-1, w)\n",
    "    grid = torch.stack([x, y], dim=2)\n",
    "    new_grid = grid + flow\n",
    "    x_new, y_new = new_grid[..., 0], new_grid[..., 1]\n",
    "    # wrap x and y:\n",
    "    # x_new = (x_new + w).fmod(w)\n",
    "    # y_new = (y_new + h).fmod(h)\n",
    "    x_new = x_new.clamp(1, w-1)\n",
    "    y_new = y_new.clamp(1, h-1)\n",
    "    x0, x1 = x_new.floor().long(), x_new.ceil().long()\n",
    "    y0, y1 = y_new.floor().long(), y_new.ceil().long()\n",
    "    \n",
    "    Ia = image[y0, x0]\n",
    "    Ib = image[y1, x0]\n",
    "    Ic = image[y0, x1]\n",
    "    Id = image[y1, x1]\n",
    "    \n",
    "    wa = (x1 - x_new).unsqueeze(2) * (y1 - y_new).unsqueeze(2)\n",
    "    wb = (x1 - x_new).unsqueeze(2) * (y_new - y0).unsqueeze(2)\n",
    "    wc = (x_new - x0).unsqueeze(2) * (y1 - y_new).unsqueeze(2)\n",
    "    wd = (x_new - x0).unsqueeze(2) * (y_new - y0).unsqueeze(2)\n",
    "    \n",
    "    warped_image = wa * Ia + wb * Ib + wc * Ic + wd * Id\n",
    "    return warped_image\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "prev_image = None\n",
    "all_images = []\n",
    "\n",
    "def cb(step: int, timestep: int, latents: torch.FloatTensor):\n",
    "    global prev_image, all_images\n",
    "\n",
    "    image = pipe.vae.decode(latents / pipe.vae.config.scaling_factor, return_dict=False)[0]\n",
    "    image = pipe.image_processor.postprocess(image, output_type=\"pt\", do_denormalize=None)\n",
    "    \n",
    "    if prev_image != None:\n",
    "        for i in range(len(image)):\n",
    "            image1 = prev_image[i]\n",
    "            image2 = image[i]\n",
    "            w, h = latents.shape[-2:]\n",
    "\n",
    "            image1_p, image2_p = preprocess(image1, image2)\n",
    "            with torch.no_grad():\n",
    "                flow = flow_model(image1_p, image2_p)[flow_stage_num]\n",
    "            flow = vision_F.resize(flow, size=[w, h], interpolation=Image.BILINEAR)[0]\n",
    "            warped_latents = warp_image(latents[i].permute(1, 2, 0), flow.permute(1, 2, 0))\n",
    "\n",
    "            ratio = timestep / 1000\n",
    "            latents[i] = latents[i] * (1 - ratio) + ratio * warped_latents.permute(2, 0, 1)\n",
    "    elif False:\n",
    "        for i in range(len(image)):\n",
    "            ratio = .1\n",
    "            encoded = pipe.vae.tiled_encode(in_image.unsqueeze(0))[0].mean\n",
    "            print(f'{encoded.shape}')\n",
    "            latents[i] = latents[i] * (1- ratio) + ratio * encoded\n",
    "\n",
    "    prev_image = image\n",
    "\n",
    "    all_images.append(image)\n",
    "\n",
    "\n",
    "prompt = 'alien, H.R. Giger, warp records'\n",
    "out = pipe(prompt, 512, 1024, callback=cb)\n",
    "out[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_images' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/tim.kuipers/dev/diffusers/src/first_try.ipynb Cell 4\u001b[0m line \u001b[0;36m8\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2231302e3230302e3130302e3635222c2275736572223a2274696d2e6b756970657273227d/home/tim.kuipers/dev/diffusers/src/first_try.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mIPython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdisplay\u001b[39;00m \u001b[39mimport\u001b[39;00m HTML\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2231302e3230302e3130302e3635222c2275736572223a2274696d2e6b756970657273227d/home/tim.kuipers/dev/diffusers/src/first_try.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m# Create a torch tensor (batch_size, height, width, channels)\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2231302e3230302e3130302e3635222c2275736572223a2274696d2e6b756970657273227d/home/tim.kuipers/dev/diffusers/src/first_try.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m batch \u001b[39m=\u001b[39m [vision_F\u001b[39m.\u001b[39mresize(ims[\u001b[39m0\u001b[39m], size\u001b[39m=\u001b[39m[\u001b[39m256\u001b[39m, \u001b[39m512\u001b[39m], antialias\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\u001b[39m.\u001b[39mpermute(\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy() \u001b[39mfor\u001b[39;00m ims \u001b[39min\u001b[39;00m all_images]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2231302e3230302e3130302e3635222c2275736572223a2274696d2e6b756970657273227d/home/tim.kuipers/dev/diffusers/src/first_try.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m fig, ax \u001b[39m=\u001b[39m plt\u001b[39m.\u001b[39msubplots()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2231302e3230302e3130302e3635222c2275736572223a2274696d2e6b756970657273227d/home/tim.kuipers/dev/diffusers/src/first_try.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m im \u001b[39m=\u001b[39m ax\u001b[39m.\u001b[39mimshow(batch[\u001b[39m0\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'all_images' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "import numpy as np\n",
    "from IPython.display import HTML\n",
    "\n",
    "# Create a torch tensor (batch_size, height, width, channels)\n",
    "batch = [vision_F.resize(ims[0], size=[256, 512], antialias=False).permute(1, 2, 0).cpu().numpy() for ims in all_images]\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(batch[0])\n",
    "\n",
    "def update(frame):\n",
    "    im.set_array(batch[frame])\n",
    "    return [im]\n",
    "\n",
    "ani = FuncAnimation(fig, update, frames=range(len(batch)), blit=True)\n",
    "HTML(ani.to_jshtml())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffusers_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
